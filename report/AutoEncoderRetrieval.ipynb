{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82efa9bb-b57b-4473-aad9-55657dfead28",
   "metadata": {},
   "source": [
    "# Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c771297-d257-48c5-b8fd-c1d156fcad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install torch torchvision torchaudio\n",
    "#conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c398a-e5e0-40f6-80da-690995306683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Dependencies\n",
    "!pip install barbar torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71be26-90a3-4e84-835e-5cd4de186a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7964d20d-7c47-4002-8269-00136d6292a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful packages to load\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from barbar import Bar\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import gc\n",
    "RANDOMSTATE = 0\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a554781-0429-4043-8a62-b74ad5e4db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2d0c2-7c80-4bf6-9ae0-7841798b6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4d357-3b64-4b15-a31d-2d3d9301c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./clean_data.csv\" #directory for data\n",
    "df = pd.read_csv(path)\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191304cc-accb-4342-a9f8-aad5bb3a55ec",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adff2cc-a0cc-42b6-8023-8da4940b35b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preparing intermediate DataFrame\n",
    "path = Path('./Data/N_train_images_model/')\n",
    "train_df  = pd.DataFrame()\n",
    "train_df['image'] = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "train_df['image'] = './Data/N_train_images_model/' + train_df['image'].astype(str)\n",
    "\n",
    "#delete incomplete image files that cannot be opened\n",
    "def delete_wrong_dim(df):\n",
    "    wrong_dim = []\n",
    "    m = 0\n",
    "    for i in df['image']:\n",
    "            m += 1\n",
    "            with open(i, 'rb') as f:\n",
    "                check_chars = f.read()[-2:]\n",
    "            if check_chars != b'\\xff\\xd9':\n",
    "                print('Not complete image')\n",
    "                x = df[df['image']== i].index.values[0]\n",
    "                wrong_dim.append(x)\n",
    "                print(x)\n",
    "        \n",
    "    df.info()\n",
    "    df = df.drop(wrong_dim)\n",
    "    df.info()\n",
    "    return df\n",
    "\n",
    "def add_class(inp_df):\n",
    "    inp_df['id'] = inp_df.image.str.split(\"/\", expand = True)[3].str.strip('.jpg')\n",
    "    inp_df['landmark_id'] = inp_df['id'].map(df.set_index(['id'])['landmark_id'])\n",
    "    return inp_df\n",
    "\n",
    "train_df = delete_wrong_dim(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d948ea-51e1-4ccd-be46-f161bb12d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_class(train_df)\n",
    "train_df = train_df[['image', 'landmark_id']]\n",
    "train_df.landmark_id = train_df['landmark_id'].astype('category')\n",
    "from sklearn import preprocessing\n",
    "# Let's try again with labels\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "train_df['landmark_id'] = label_encoder.fit_transform(train_df['landmark_id'])\n",
    "y_train = train_df['landmark_id']\n",
    "y_train\n",
    "train_df = train_df[['image']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ad572-0a63-4aaf-8747-31372549e6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path('./Data/M_validation_images_model/')\n",
    "val_df  = pd.DataFrame()\n",
    "val_df['image'] = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "val_df['image'] = './Data/M_validation_images_model/' + val_df['image'].astype(str)\n",
    "\n",
    "val_df.head()\n",
    "val_df = delete_wrong_dim(val_df)\n",
    "#val_df = add_class(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae85c2-e6f9-4866-a261-3d8b566db070",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e577ec-0ae8-4df8-80a2-5caf62451fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path('./Data/N_test_images_from_train/')\n",
    "test_df  = pd.DataFrame()\n",
    "test_df['image'] = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "test_df['image'] = './Data/N_test_images_from_train/' + test_df['image'].astype(str)\n",
    "test_df = delete_wrong_dim(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965687e-4b33-45bc-8a5e-013deb49e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBIRDataset(Dataset):\n",
    "    def __init__(self, dataFrame):\n",
    "        self.dataFrame = dataFrame\n",
    "        \n",
    "        self.transformations = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((92, 92), antialias=True)\n",
    "            ,transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            raise NotImplementedError('slicing is not supported')\n",
    "        \n",
    "        row = self.dataFrame.iloc[key]\n",
    "        image = self.transformations(Image.open(row['image']).convert('RGB'))\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataFrame.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e83d6-6d9e-4fc5-8a7f-1b4b313b317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Function to process data from the data retrival class\n",
    "def prepare_data(trainDF,validateDF,testDF):\n",
    "    train_set = CBIRDataset(trainDF)\n",
    "    validate_set = CBIRDataset(validateDF)\n",
    "    test_set = CBIRDataset(testDF)\n",
    "    return train_set, validate_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da7fb2-cb5e-41f0-8c10-e3550bff4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validate_set, test_set = prepare_data(train_df, val_df, test_df) #call CBIRDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283f764-77ad-4736-be61-eb633716527f",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f30ed-1558-452e-b9be-89bbbf7493fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvAutoencoderB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoderB, self).__init__()\n",
    "        self.encoder = nn.Sequential(# in- (N,3,92,92)\n",
    "            # M = floor(M - K + 2*P)/S + 1 = 92 - 3 + 2 /2  + 1 = 46\n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=64, \n",
    "                      kernel_size=(3,3), \n",
    "                      stride=2, \n",
    "                      padding=1),  # (32,16,46,46)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (N,16,23,23)  46 - 2  /2 + 1= 23\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, \n",
    "                      out_channels=32, \n",
    "                      kernel_size=(3,3), \n",
    "                      stride=2, \n",
    "                      padding=1),  # (N,8,43,43) 23 -  3 + 2  /  2 + 1 \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (N,8,11,11) 12-2 /1 + 1 = 11\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            #strides * (input_size-1) + kernel_size - 2*padding\n",
    "            nn.ConvTranspose2d(in_channels = 32, \n",
    "                               out_channels=16, \n",
    "                               kernel_size=(5,5), \n",
    "                               stride=2,\n",
    "                              padding=1),  # (N,16,25,25) 2* (11-1) + 5 -2 = 23\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(in_channels=16, \n",
    "                               out_channels=8, \n",
    "                               kernel_size=(3,3), \n",
    "                               stride=2, \n",
    "                               padding=1),  # (N,8,48,48) 2 *(23-1) + 3 - 2 = 45\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ConvTranspose2d(in_channels=8, \n",
    "                               out_channels=3, \n",
    "                               kernel_size=(6,6), \n",
    "                               stride=2, \n",
    "                               padding=1),  # (N,3,512,512) > 2*(45-1) + 6 -2 = 92\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8616fd5-886d-4ac9-a180-8f5192594a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvAutoencoderB2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoderB2, self).__init__()\n",
    "        self.encoder = nn.Sequential(# in- 92\n",
    "            \n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=64, \n",
    "                      kernel_size=(3,3), \n",
    "                      stride=3, \n",
    "                      padding=1),  # \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, \n",
    "                      out_channels=32, \n",
    "                      kernel_size=(3,3), \n",
    "                      stride=2, \n",
    "                      padding=1),  # (N,32,43,43)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (N,32,42,42)\n",
    "        \n",
    "            \n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels = 32, \n",
    "                               out_channels=16, \n",
    "                               kernel_size=(3,3), \n",
    "                               stride=2),  # (N,16,85,85)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(in_channels=16, \n",
    "                               out_channels=8, \n",
    "                               kernel_size=(5,5), \n",
    "                               stride=3, \n",
    "                               padding=1),  # (N,8,255,255)\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ConvTranspose2d(in_channels=8, \n",
    "                               out_channels=3, \n",
    "                               kernel_size=(6,6), \n",
    "                               stride=2, \n",
    "                               padding=1),  # (N,3,512,512)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e9bd0-8cc7-4c40-9257-ce7a7ef79d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
    "            \n",
    "            nn.Conv2d(in_channels=3, \n",
    "                      out_channels=16, \n",
    "                      kernel_size=(3,3), \n",
    "                      stride=3, \n",
    "                      padding=1),  # (32,16,171,171)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, \n",
    "                      out_channels=8, \n",
    "                      kernel_size=(3,3), \n",
    "                      stride=2, \n",
    "                      padding=1),  # (N,8,43,43)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels = 8, \n",
    "                               out_channels=16, \n",
    "                               kernel_size=(3,3), \n",
    "                               stride=2),  # (N,16,85,85)\n",
    "            nn.ReLU(True),\n",
    " \n",
    "            nn.ConvTranspose2d(in_channels=16, \n",
    "                               out_channels=8, \n",
    "                               kernel_size=(5,5), \n",
    "                               stride=3, \n",
    "                               padding=1),  # (N,8,255,255)\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=8, \n",
    "                               out_channels=3, \n",
    "                               kernel_size=(6,6), \n",
    "                               stride=2, \n",
    "                               padding=1),  # (N,3,512,512)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7669cb1-4c91-434d-8946-657e35862e5a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de54788-d600-411c-a2eb-8a6af9ed8572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training losses\n",
    "\n",
    "def load_ckpt(checkpoint_fpath, model, optimizer):\n",
    "    \n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    #valid_loss_min = checkpoint['valid_loss_min']\n",
    "\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    print (\"=> Saving a new best\")\n",
    "    torch.save(state, filename)  # save checkpoint\n",
    "\n",
    "\n",
    "def train_model(model,  \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                num_epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                \n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for idx,inputs in enumerate(Bar(dataloaders[phase])): \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)\n",
    "        \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "           \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                train_cost.append(epoch_loss)\n",
    "            else:\n",
    "                val_cost.append(epoch_loss)\n",
    "                \n",
    "                     \n",
    "            print('{} Loss: {:.4f}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                save_checkpoint(state={   \n",
    "                                    'epoch': epoch,\n",
    "                                    'state_dict': model.state_dict(),\n",
    "                                    'best_loss': best_loss,\n",
    "                                    'optimizer_state_dict':optimizer.state_dict()\n",
    "                                },filename='./result/ckpt_epoch_{}.pt'.format(epoch))\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e1d73-eecd-4743-92f6-bf07900aa943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "NUM_BATCHES = 32\n",
    "RETRAIN = False\n",
    "\n",
    "dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=0),\n",
    "                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=0)\n",
    "                }\n",
    "\n",
    "dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n",
    "              \n",
    "model = ConvAutoencoderB().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "train_cost=[]\n",
    "val_cost=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814c832-7b98-48eb-aea7-cdc8a865603d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, optimizer, loss = train_model(model=model, \n",
    "                    criterion=criterion, \n",
    "                    optimizer=optimizer, \n",
    "                    num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db463c-f826-49f1-9088-faead50c97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "plt.plot(train_cost)\n",
    "plt.plot(val_cost)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b15370-fc03-43bb-b2b5-487567ca3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "print(\"Test Images\")\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    pic = test_set[i].numpy().transpose((1,2,0))\n",
    "    pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "    plt.imshow(pic)\n",
    "plt.show()\n",
    "\n",
    "# model(test_set[0])\n",
    "plt.figure(figsize=(20, 4))\n",
    "print(\"Reconstruction of Test Images\")\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    pic = model(test_set[i].unsqueeze(0)).squeeze(0).detach().numpy().transpose((1,2,0))\n",
    "    pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
    "    plt.imshow(pic)\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c332b-bdb4-4423-b3dd-9bd78e49e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "torch.save({\n",
    "            'epoch': EPOCHS,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, './cbirpretrained/conv_autoencoderBatchNorm_B_50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35106a-a191-4057-9590-6b5efbceee72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If re-training is required:\n",
    "# Load the old model\n",
    "if RETRAIN == True:\n",
    "    load the saved checkpoint\n",
    "    model, optimizer, start_epoch = load_ckpt('./cbirpretrained/conv_autoencoderBatchNorm_B_20.pt', model, optimizer)\n",
    "    print('Checkpoint Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36925d-b429-4e40-8fad-e43451d57f48",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2a7e5-9064-47c1-a3f6-60ed2cb91cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 92\n",
    "transformations = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((size, size), antialias=True),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f163d9-2675-4092-af7a-b258cb986a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Model in Evaluation phase\n",
    "model = ConvAutoencoderB().to(device)\n",
    "#replace the model's parameters with those of the trained net\n",
    "model.load_state_dict(torch.load('./cbirpretrained/conv_autoencoderBatchNorm_B_20.pt', map_location=device)['model_state_dict'], strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc4391-3434-444f-9b22-b852df76005b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get laten features = take the output encoder part of the pretrained model \n",
    "num_img = len(train_df) \n",
    "out_dim = 11\n",
    "num_output = 32\n",
    "def get_latent_features(images, transformations):\n",
    "    latent_features = np.zeros((num_img,num_output,out_dim,out_dim)) # M in last layer of encoder()\n",
    "    \n",
    "    for i,image in enumerate(tqdm(images)):\n",
    "        tensor = transformations(Image.open(image).convert('RGB')).to(device)\n",
    "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
    "        \n",
    "    del tensor\n",
    "    gc.collect()\n",
    "    return latent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea29b0-a7ae-43dc-b4d2-2fbf7029796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_df.image.values\n",
    "latent_feature = get_latent_features(images, transformations)\n",
    "print(images.shape)\n",
    "print(latent_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66add744-6c7f-4550-889a-366acbd9c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.AdaptiveAvgPool2d(1)\n",
    "latent_feature = torch.tensor(latent_feature)\n",
    "latent_feature.shape\n",
    "pred = m(latent_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f1734-ba73-4787-8462-a0c1e0f37c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= pred.view(pred.shape[:2])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55cdd5-b97d-4595-8370-9b8610d75110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "classifier = KMeans(n_clusters=39, random_state=0).fit(pred)\n",
    "classifier.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed542b73-c967-405c-8dbb-d1edcc2765c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb478d1b-6605-40ae-8507-2f6cd87ddfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "nmis = normalized_mutual_info_score(y_train, classifier.labels_)\n",
    "nmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc2c59-50eb-4291-b2f0-c5c873689d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "def rand_index_score(clusters, classes):\n",
    "    tp_plus_fp = comb(np.bincount(clusters), 2).sum()\n",
    "    tp_plus_fn = comb(np.bincount(classes), 2).sum()\n",
    "    A = np.c_[(clusters, classes)]\n",
    "    tp = sum(comb(np.bincount(A[A[:, 0] == i, 1]), 2).sum()\n",
    "             for i in set(clusters))\n",
    "    fp = tp_plus_fp - tp\n",
    "    fn = tp_plus_fn - tp\n",
    "    tn = comb(len(A), 2) - tp - fp - fn\n",
    "    return (tp + tn) / (tp + fp + fn + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c018b8-3517-4441-bb59-f39c2b616801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri = rand_index_score (y_train, classifier.labels_)\n",
    "ri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9fcab-73de-4cdb-8d83-a34a64685176",
   "metadata": {},
   "source": [
    "# 2. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e262f-ec9e-4e6f-8656-2a95a98dee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = len(train_df) + len(val_df)\n",
    "out_dim = 11\n",
    "num_output = 32\n",
    "def get_latent_features(images, transformations):\n",
    "    latent_features = np.zeros((num_img,num_output,out_dim,out_dim)) # M in last layer of encoder()\n",
    "    \n",
    "    for i,image in enumerate(tqdm(images)):\n",
    "        tensor = transformations(Image.open(image).convert('RGB')).to(device)\n",
    "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
    "        \n",
    "    del tensor\n",
    "    gc.collect()\n",
    "    return latent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6cfe9-8ad4-4b4e-8c3e-7103a99c2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, val_df])\n",
    "images = df.image.values\n",
    "latent_features = get_latent_features(images, transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b7030-5ba2-4d48-92f5-f372e939f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(0, num_img))\n",
    "feature_dict = dict(zip(indexes,latent_features))\n",
    "index_dict = {'indexes':indexes,'features':latent_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fd9f8-3bf4-469b-b925-c5e41acbff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(a, b):\n",
    "    # compute and return the euclidean distance between two vectors\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56919c8e-7bfb-4155-ab81-f5dfeef1411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(queryFeatures, index, maxResults=64):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(index[\"features\"])):\n",
    "        # compute the euclidean distance between our query features\n",
    "        # and the features for the current image in our index, then\n",
    "        # update our results list with a 2-tuple consisting of the\n",
    "        # computed distance and the index of the image\n",
    "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
    "        results.append((d, i))\n",
    "    \n",
    "    # sort the results and grab the top ones\n",
    "    results = sorted(results)[:maxResults]\n",
    "    # return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f673e1e-b36a-43fe-8b52-d233dd64af2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take the features for the current image, find all similar\n",
    "# images in our dataset, and then initialize our list of result\n",
    "queryIdx =2000 # Input Index for which images #800, 2000, 5490,6789\n",
    "MAX_RESULTS = 5\n",
    "\n",
    "queryFeatures = latent_features[queryIdx]\n",
    "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS+1)\n",
    "imgs = []\n",
    "img_name = []\n",
    "# loop over the results\n",
    "for (d, j) in results:\n",
    "    if j != queryIdx:\n",
    "        img = np.array(Image.open(images[j]))\n",
    "        img_name.append(j)\n",
    "        imgs.append(img)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "gs = fig.add_gridspec(2,MAX_RESULTS)\n",
    "\n",
    "# display the query image\n",
    "#plt.gca().set_title('query image')\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Query Image idx_' + str(queryIdx))\n",
    "ax0.imshow(np.array(Image.open(images[queryIdx])))\n",
    "\n",
    "for i in range(MAX_RESULTS):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    plt.title('idx_' + str(img_name[i]))\n",
    "    plt.axis('off')\n",
    "    ax.imshow(imgs[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2396e-563d-46da-9806-b01272e0b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = len(test_df)\n",
    "testimages = test_df.image.values\n",
    "test_latent_features = get_latent_features(testimages, transformations)\n",
    "indexes = list(range(0, num_img))\n",
    "feature_dict = dict(zip(indexes,test_latent_features))\n",
    "index_dict = {'indexes':indexes,'features':test_latent_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06ca13-9936-445a-9b07-ff336d357715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(2,figsize=(15,15))\n",
    "MAX_RESULTS = 5\n",
    "queryIdx = 34 #between 0 and 65\n",
    "\n",
    "queryFeatures = test_latent_features[queryIdx]\n",
    "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS+1)\n",
    "imgs = []\n",
    "img_name = []\n",
    "\n",
    "# loop over the results\n",
    "for (d, j) in results:\n",
    "    if j != queryIdx:\n",
    "        img = np.array(Image.open(testimages[j]))\n",
    "        img_name.append(j)\n",
    "        imgs.append(img)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "gs = fig.add_gridspec(2,5)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "plt.axis('off')\n",
    "plt.title('Query Image idx_' + str(queryIdx))\n",
    "ax0.imshow(np.array(Image.open(testimages[queryIdx])))\n",
    "\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    plt.title('idx_' + str(img_name[i]))\n",
    "    plt.axis('off')\n",
    "    ax.imshow(imgs[i])\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
